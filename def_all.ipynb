{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 寫成def參數一次爬取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#依順序import進module\n",
    "import sys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "def craw_all(i): \n",
    "    sys.stdout.write('正在抓取捐款進度報告第'+ '%s'%i +'頁')\n",
    "    craw = requests.get('https://tw.appledaily.com/charity/projlist/Page/%s'%i)\n",
    "    soup = BeautifulSoup(craw.text, 'lxml')\n",
    "    #抓取220個網頁中每一張捐款進度報告表\n",
    "    all_cells = soup.find_all(re.compile(\"td\"))\n",
    "    all_cells[:6] \n",
    "    list = []\n",
    "    for s in all_cells[6:]:   \n",
    "        data = s.text\n",
    "        list.append(data)\n",
    "    matrix=[]                        #或可先轉為ndarry再reshape  np.array(list).reshape\n",
    "    for m in range(0,len(list),6):\n",
    "        matrix.append(list[m:m+6])   \n",
    "    matrix=np.array(matrix) \n",
    "    matrixT=matrix.T\n",
    "    tables=pd.DataFrame()  \n",
    "    for t in range(0,len(matrixT)):\n",
    "        tables['%s'%t]=matrixT[t]\n",
    "    #存成表格\n",
    "    tables = tables.rename(columns={'0':'編號','1':'報導標題','2':'刊登日期','3':'狀態','4':'累計(元)','5':'捐款明細'})\n",
    "    #加入url位址\n",
    "    article_url = [tag.find(\"a\")['href'] for tag in soup.find_all(\"h2\")[3:]]  \n",
    "    tables['text_url'] = pd.Series(article_url)  \n",
    "    web_url = 'https://tw.appledaily.com/'  \n",
    "    detail_url = [web_url + tag['href'] for tag in soup.find_all(\"a\", {\"class\":\"details\"})] \n",
    "    tables['detail_url'] = pd.Series(detail_url) \n",
    "    #存檔輸出表格\n",
    "    tables.to_excel(\"./my_crawler_all/reports/report_%s\"%i+\".xlsx\")\n",
    "\n",
    "    #將每一張表格中的每一則報導全文存檔輸出 或with open(filename,'w')\n",
    "    for url in article_url:\n",
    "        res = requests.get(url)\n",
    "        soup = BeautifulSoup(res.text, \"lxml\")\n",
    "        main_title = soup.find(\"article\", {\"class\":\"ndArticle_leftColumn\"}).find(\"h1\").text   \n",
    "        sub_title = soup.find(\"article\", {\"class\":\"ndArticle_leftColumn\"}).find(\"h2\").text \n",
    "        paragraph = [tag.text for tag in soup.find(\"div\", {\"class\":\"ndArticle_margin\"}).find_all(\"p\")]\n",
    "        df = pd.DataFrame()\n",
    "        df['text']=pd.Series(paragraph)\n",
    "        df.to_csv(\"./my_crawler_all/articles/\"+\"page%s\"%i+\"-\"+url.split('/')[-2]+\"-\"+url.split('/')[-1]+\".csv\",encoding='utf_8_sig') #bylines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在抓取捐款進度報告第101頁正在抓取捐款進度報告第102頁正在抓取捐款進度報告第103頁正在抓取捐款進度報告第104頁正在抓取捐款進度報告第105頁正在抓取捐款進度報告第106頁正在抓取捐款進度報告第107頁正在抓取捐款進度報告第108頁正在抓取捐款進度報告第109頁正在抓取捐款進度報告第110頁正在抓取捐款進度報告第111頁正在抓取捐款進度報告第112頁正在抓取捐款進度報告第113頁正在抓取捐款進度報告第114頁正在抓取捐款進度報告第115頁正在抓取捐款進度報告第116頁正在抓取捐款進度報告第117頁正在抓取捐款進度報告第118頁正在抓取捐款進度報告第119頁正在抓取捐款進度報告第120頁"
     ]
    }
   ],
   "source": [
    "for i in range(101, 121):\n",
    "    craw_all(i)   #指定範圍, 爬取中間20頁的表格與約20*20篇全文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, 221):\n",
    "    craw_all(i)   #全數輸出    把Wins上的檔案以linux aws3指令搬到aws平臺上,利用EC2演算法平臺將py寫入sh陸續進行文章斷詞與探勘分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
